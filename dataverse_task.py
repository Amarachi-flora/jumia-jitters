# =======================================
# Dataverse Africa July Challenge Project
# Theme: E-Commerce Logistics & Seller Risk Profiling
# =======================================

import sys
import os

# Create a logs folder if it doesn't exist
os.makedirs("logs", exist_ok=True)

# Redirect stdout and stderr to a file
sys.stdout = open("logs/output_log.txt", "w", encoding="utf-8")
sys.stderr = sys.stdout  # Optional: capture error messages too


# =======================================
#  Task 1: Data Cleaning & Feature Engineering
# =======================================
# Objective: Clean and standardize the dataset, handle missing values, 
# and engineer features such as sentiment label, delivery delay, and seller risk scores.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import f_oneway
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from scipy.sparse import hstack, csr_matrix
import os
import warnings
import matplotlib
matplotlib.use('TkAgg')



# Load the dataset
file_path = 'messey_jumia_jitters_dataset.xlsx'
df = pd.read_excel(file_path)

print("\n RAW Data Sample:")
print(df.head(5))
print("\n RAW Columns:")
print(df.columns.tolist())


#  Standardize column names
df.columns = df.columns.str.lower().str.replace(' ', '_')

#  Fix missing values
df['order_date'] = df['order_date'].ffill()
df['dispatch_date'] = df['dispatch_date'].ffill()
df['delivery_date'] = df['delivery_date'].ffill()
df['product_category'] = df['product_category'].fillna('Unknown')
df['price'] = df['price'].fillna(df['price'].median())
df['quantity'] = df['quantity'].fillna(df['quantity'].median())
df['warehouse_zone'] = df['warehouse_zone'].fillna('Unknown')
df['customer_rating'] = df['customer_rating'].fillna(df['customer_rating'].mean())
df['sentiment_score'] = df['sentiment_score'].fillna(df['sentiment_score'].mean())
df['complaint_code'] = df['complaint_code'].fillna('None')
df['review_text'] = df['review_text'].fillna('').replace('', 'No review')

#  Clean duplicate columns
if 'product_pategory' in df.columns:
    df.drop(columns=['product_pategory'], inplace=True)

#  Remove duplicated column names
df = df.loc[:, ~df.columns.duplicated()]

#  Sentiment Label

def sentiment_label(score):
    if score > 0.1:
        return 'Positive'
    elif score < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment_label'] = df['sentiment_score'].apply(sentiment_label)

#  String normalization
text_cols = ['product_category', 'warehouse_zone', 'customer_region', 'complaint_code']
for col in text_cols:
    df[col] = df[col].astype(str).str.title().str.strip()

#  Delivery Delay Feature
df['delivery_delay'] = (df['delivery_date'] - df['dispatch_date']).dt.days

#  Flag suspicious reviews
df['review_length'] = df['review_text'].apply(len)
df['duplicate_review'] = df.duplicated(subset=['review_text'])
df['suspicious_review'] = np.where((df['review_length'] < 10) | (df['duplicate_review']), True, False)

#  Seller-Level Aggregations
seller_group = df.groupby('seller_id').agg({
    'return_flag': lambda x: (x == 'Yes').mean(),
    'customer_rating': 'mean',
    'complaint_code': lambda x: x.notnull().mean()
}).rename(columns={
    'return_flag': 'return_rate',
    'customer_rating': 'average_rating',
    'complaint_code': 'complaint_rate'
}).reset_index()

#  Add average delivery delay per seller
seller_delay = df.groupby('seller_id')['delivery_delay'].mean().reset_index()
seller_delay.rename(columns={'delivery_delay': 'avg_delay'}, inplace=True)
seller_group = pd.merge(seller_group, seller_delay, on='seller_id', how='left')

#  Compute Seller Risk Score
seller_group['seller_risk_score'] = (
    seller_group['return_rate'] * 0.35 +
    seller_group['complaint_rate'] * 0.35 +
    (1 - seller_group['average_rating'] / 5) * 0.15 +
    (seller_group['avg_delay'] / df['delivery_delay'].max()) * 0.15
)

#  Merge back to main df
df = pd.merge(df, seller_group, on='seller_id', how='left')

#  Save cleaned dataset
df.to_csv('cleaned_jumia_data.csv', index=False)
seller_group.to_csv('seller_risk_summary.csv', index=False)

print("\n CLEANED Data Sample:")
print(df.head(5))
print("\n CLEANED Columns:")
print(df.columns.tolist())


print(" Task 1 Complete: Data cleaned and risk score engineered")



# =======================================
#  Task 2: Pattern Surveillance - Visual Analysis
# =======================================
# Objective: To generate visual insights from the dataset to support business decisions.

sns.set(style="whitegrid")

#  Chart 1: Distribution of Delivery Methods
plt.figure(figsize=(8, 5))
ax = sns.countplot(data=df, x='delivery_method', hue='delivery_method', palette='Set2', legend=False)
plt.title("Distribution of Delivery Methods")
plt.xlabel("Delivery Method")
plt.ylabel("Number of Orders")
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., p.get_height()), ha='center', va='bottom')
plt.tight_layout()
plt.show()

# Objective: Understand preferred logistics methods.
# Insight: Dominance of certain delivery methods.
# Recommendation: Optimize resource allocation based on preferred method.


#  Chart 2: Top Ordered Product Categories
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df, y='product_category', hue='product_category', order=df['product_category'].value_counts().index, palette='Set3', legend=False)
plt.title("Top Ordered Product Categories")
plt.xlabel("Number of Orders")
plt.ylabel("Product Category")
for p in ax.patches:
    ax.annotate(f'{p.get_width()}', (p.get_width()+3, p.get_y()+0.4))
plt.tight_layout()
plt.show()

# Objective: Identify best-selling product types.
# Insight: A few categories dominate order volume.
# Recommendation: Focus marketing and stock planning around top categories.


#  Chart 3: Customer Region Distribution
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df, x='customer_region', hue='customer_region', order=df['customer_region'].value_counts().index, palette='coolwarm', legend=False)
plt.title("Customer Region Distribution")
plt.xlabel("Region")
plt.ylabel("Number of Orders")
plt.xticks(rotation=45)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., p.get_height()), ha='center', va='bottom')
plt.tight_layout()
plt.show()

# Objective: Locate concentration of customer base.
# Insight: Some regions have significantly higher demand.
# Recommendation: Enhance logistics and marketing efforts in top regions.


#  Chart 4: Complaint Count by Product Category
complaints = df[df['complaint_code'] != 'None']['product_category'].value_counts().reset_index()
complaints.columns = ['product_category', 'complaint_count']
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=complaints, y='product_category', x='complaint_count', hue='product_category', palette='flare', legend=False)
plt.title("Complaints by Product Category")
plt.xlabel("Number of Complaints")
plt.ylabel("Product Category")
for p in ax.patches:
    ax.annotate(f'{int(p.get_width())}', (p.get_width() + 1, p.get_y() + 0.4))
plt.tight_layout()
plt.show()

# Objective: Uncover problematic product types.
# Insight: Certain categories are complaint-prone.
# Recommendation: Conduct product quality checks on flagged categories.


#  Chart 5: Sentiment Score Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['sentiment_score'], bins=30, kde=True, color='skyblue')
plt.title("Distribution of Sentiment Scores")
plt.xlabel("Sentiment Score")
plt.ylabel("Number of Reviews")
plt.tight_layout()
plt.show()

# Objective: Understand customer satisfaction distribution.
# Insight: Majority scores lean positive/neutral.
# Recommendation: Monitor sentiment dips to address customer pain points.


#  Chart 6: Return vs Non-Return Analysis
plt.figure(figsize=(6, 4))
ax = sns.countplot(data=df, x='return_flag', hue='return_flag', palette='pastel', legend=False)
plt.title("Return Flag Distribution")
plt.xlabel("Return Status")
plt.ylabel("Count")
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., p.get_height()), ha='center', va='bottom')
plt.tight_layout()
plt.show()

# Objective: Measure frequency of returns.
# Insight: Very low return rate.
# Recommendation: Maintain or improve return-related logistics and product quality.


#  Chart 7: Average Rating by Product Category
rating_cat = df.groupby('product_category')['customer_rating'].mean().sort_values(ascending=False).reset_index()
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=rating_cat, x='customer_rating', y='product_category', hue='product_category', palette='Blues_d', legend=False)
plt.title("Average Rating per Product Category")
plt.xlabel("Average Rating")
plt.ylabel("Product Category")
for p in ax.patches:
    ax.annotate(f'{p.get_width():.2f}', (p.get_width() + 0.05, p.get_y() + 0.4))
plt.tight_layout()
plt.show()

# Objective: Identify customer satisfaction by product type.
# Insight: Some categories are rated significantly higher.
# Recommendation: Promote top-rated categories and investigate low-rated ones.


#  Chart 8: Sentiment Level Breakdown
df['sentiment_level'] = df['sentiment_score'].apply(sentiment_label)
plt.figure(figsize=(6, 4))
ax = sns.countplot(data=df, x='sentiment_level', hue='sentiment_level', order=['Positive', 'Neutral', 'Negative'], palette='cool', legend=False)
plt.title("Sentiment Level Breakdown")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width()/2., p.get_height()), ha='center', va='bottom')
plt.tight_layout()
plt.show()


# Objective: Segment customer sentiment clearly.
# Insight: Positive sentiments dominate.
# Recommendation: Use positive feedback in marketing and resolve negative feedback efficiently.


#  Chart 9: Complaint Heatmap by Product Category & Region
heatmap_data = df.pivot_table(index='product_category', columns='customer_region', values='complaint_code', aggfunc='count', fill_value=0)
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu', linewidths=0.5, linecolor='gray', cbar_kws={'label': 'Complaint Count'})
plt.title('Complaint Distribution Heatmap by Product Category and Customer Region', fontsize=14)
plt.xlabel('Customer Region', fontsize=12)
plt.ylabel('Product Category', fontsize=12)
plt.tight_layout()
plt.show()

# Objective: Spot clusters of complaint issues.
# Insight: Specific regions & categories drive complaints.
# Recommendation: Launch targeted audits or training for sellers in problem areas.


#  Chart 10: Average Delivery Delay per Region
avg_delay = df.groupby('customer_region')['delivery_delay'].mean().sort_values(ascending=False).reset_index()
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=avg_delay, x='delivery_delay', y='customer_region', hue='customer_region', palette='magma', legend=False)
plt.title("Average Delivery Delay by Region")
plt.xlabel("Average Delivery Delay (Days)")
plt.ylabel("Customer Region")
for i, v in enumerate(avg_delay['delivery_delay']):
    ax.text(v + 0.1, i, f"{v:.1f}", color='black', va='center')
plt.tight_layout()
plt.show()

# Objective: Identify regions with the longest delivery times.
# Insight: Some regions experience significantly higher delays. Certain regions (e.g., Lagos or North-West) may experience consistent delays.
# Recommendation: Optimize route planning and warehouse placement in those regions



# =======================================
# Hypothesis Test: Does Delivery Method Affect Customer Rating?
# =======================================

groups = [grp['customer_rating'].dropna().values for name, grp in df.groupby('delivery_method')]
f_stat, p_value = f_oneway(*groups)
print("\nANOVA Result:")
print("F-statistic:", round(f_stat, 2))
print("P-value:", round(p_value, 4))
if p_value < 0.05:
    print("Conclusion: Significant difference in ratings across delivery methods.")
else:
    print("Conclusion: No statistically significant difference in ratings based on delivery method.")

print("\n Task 2 Complete: Visual insights generated.")




# =======================================
# Task 3: Prediction & Risk Modeling
# =======================================
# Objective: Predict Return Flag using selected features and identify risky sellers

#  Feature Selection
features = ['seller_id', 'product_category', 'price', 'delivery_delay', 'complaint_code']
df_model = df[features + ['return_flag']].copy()
df_model['return_flag'] = df_model['return_flag'].map({'No': 0, 'Yes': 1})

#  One-hot Encoding
X = pd.get_dummies(df_model.drop(columns='return_flag'), drop_first=True)
y = df_model['return_flag']

#  Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#  Handle Class Imbalance with SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train, y_train)

#  Train Classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_resampled, y_resampled)

#  Predict & Evaluate
y_pred = clf.predict(X_test)
proba = clf.predict_proba(X_test)[:, 1]
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, proba))

#  Add predicted probabilities for analysis
X_test = X_test.copy()
X_test['predicted_prob'] = proba
X_test['seller_id'] = df.loc[X_test.index, 'seller_id'].values

#  Identify High-Risk Sellers
risky_sellers = X_test[X_test['predicted_prob'] >= 0.7][['seller_id', 'predicted_prob']]
risky_summary = risky_sellers.groupby('seller_id')['predicted_prob'].mean().reset_index()
risky_summary.columns = ['seller_id', 'avg_return_risk']

#  Top Sellers to Investigate
top_risk_sellers = risky_summary.sort_values(by='avg_return_risk', ascending=False).head(10)
print("\nTop High-Risk Sellers:")
print(top_risk_sellers)

#  Visualize Risky Sellers
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=top_risk_sellers, x='avg_return_risk', y='seller_id', color='salmon')
plt.title('Top Predicted High-Return Sellers')
plt.xlabel('Predicted Return Risk')
plt.ylabel('Seller ID')
for i, v in enumerate(top_risk_sellers['avg_return_risk']):
    ax.text(v + 0.01, i, f"{v:.2f}", color='black', va='center')
plt.tight_layout()
plt.show()

# Save to CSV for review dashboard/table
top_risk_sellers.to_csv("sellers_to_investigate.csv", index=False)


# Task 3 Complete: Model trained, risky sellers identified



# =======================================
# NLP-enhanced Return Prediction
# =======================================
# Vectorize review text using TF-IDF
vectorizer = TfidfVectorizer(max_features=300)
review_vectors = vectorizer.fit_transform(df['review_text'])

# Create new model dataset with text vectors
df_model_nlp = df[['seller_id', 'product_category', 'price', 'delivery_delay', 'complaint_code', 'return_flag']].copy()
df_model_nlp['return_flag'] = df_model_nlp['return_flag'].map({'No': 0, 'Yes': 1})

# Encode categorical features
df_encoded = pd.get_dummies(df_model_nlp.drop(columns='return_flag'), drop_first=True)

# Concatenate TF-IDF vectors with encoded features
X_nlp = hstack([csr_matrix(df_encoded.values.astype(np.float32)), review_vectors])
y_nlp = df_model_nlp['return_flag']

# Train/test split
X_train_nlp, X_test_nlp, y_train_nlp, y_test_nlp = train_test_split(X_nlp, y_nlp, test_size=0.3, random_state=42)

# Handle imbalance
X_resampled_nlp, y_resampled_nlp = SMOTE(random_state=42).fit_resample(X_train_nlp, y_train_nlp)

# Train classifier with text features
clf_nlp = LogisticRegression(max_iter=1000, random_state=42)
clf_nlp.fit(X_resampled_nlp, y_resampled_nlp)

# Predict & evaluate
y_pred_nlp = clf_nlp.predict(X_test_nlp)
proba_nlp = clf_nlp.predict_proba(X_test_nlp)[:, 1]
print("\n NLP-Enhanced Classification Report:\n", classification_report(y_test_nlp, y_pred_nlp))
print("Confusion Matrix:\n", confusion_matrix(y_test_nlp, y_pred_nlp))
print("AUC Score:", roc_auc_score(y_test_nlp, proba_nlp))

print("\n NLP-enhanced model tested and evaluated.")



# =======================================
# A dashboard/table of sellers to suspend or investigate
# =======================================

# Step 1: Filter sellers with high average predicted return risk
# Threshold: 0.7 or higher = high risk
risky_sellers = X_test[X_test['predicted_prob'] >= 0.7][['seller_id', 'predicted_prob']]

# Step 2: Aggregate average risk per seller
risky_summary = risky_sellers.groupby('seller_id')['predicted_prob'].mean().reset_index()
risky_summary.columns = ['Seller ID', 'Average Return Risk']

# Step 3: Sort and keep top N risky sellers (e.g., top 10)
top_risk_sellers = risky_summary.sort_values(by='Average Return Risk', ascending=False).head(10)

# Step 4: Display table
print("\n Sellers to Suspend or Investigate:\n")
print(top_risk_sellers.to_string(index=False))

#  Save as CSV
top_risk_sellers.to_csv("sellers_to_investigate.csv", index=False)


# =======================================
# Bonus: Visual Dashboard
# =======================================
 
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Step 1: Prepare Seller Info (id + top product)
# we'll just use 'seller_id'

# Get top product category per seller
seller_products = df.groupby('seller_id')['product_category'].agg(lambda x: x.mode()[0]).reset_index()
seller_products.columns = ['seller_id', 'Top Product']

# seller_names = df[['seller_id', 'seller_name']].drop_duplicates()
# seller_info = pd.merge(seller_names, seller_products, on='seller_id', how='left')

# Merge product info with top risk sellers
merged_sellers = pd.merge(top_risk_sellers, seller_products, left_on='Seller ID', right_on='seller_id', how='left')
merged_sellers['Label'] = merged_sellers['Seller ID'] + " - " + merged_sellers['Top Product']

# Step 2: Plot with new label
plt.figure(figsize=(12, 6))
ax = sns.barplot(
    data=merged_sellers,
    x='Average Return Risk',
    y='Label',
    color='red'
)

plt.title('Top Sellers to Suspend or Investigate')
plt.xlabel('Predicted Return Risk')
plt.ylabel('Seller & Product')

# Annotate values
for i, v in enumerate(merged_sellers['Average Return Risk']):
    ax.text(v + 0.01, i, f"{v:.2f}", color='black', va='center')

plt.tight_layout()
plt.show()



# =======================================
#  TASK 4: Strategy & Resolution
# =======================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
import os


#  1. Recommend 5 Sellers to Suspend

top_5_sellers = df.groupby('seller_id')['seller_risk_score'].mean().reset_index()
top_5_sellers = top_5_sellers.sort_values(by='seller_risk_score', ascending=False).head(5)

print("\n 5 Sellers to Suspend (High Risk):")
print(top_5_sellers.to_string(index=False))


#  2. Product Categories to Blacklist/Regulate

complaints = df[df['complaint_code'].str.lower() != 'none']
category_complaint_rate = complaints['product_category'].value_counts(normalize=True).reset_index()
category_complaint_rate.columns = ['Product Category', 'Complaint Rate']
blacklist_categories = category_complaint_rate[category_complaint_rate['Complaint Rate'] > 0.05]

print("\n Product Categories to Blacklist or Regulate:")
print(blacklist_categories.to_string(index=False))


# VISUALIZATION

# Set style
sns.set(style="whitegrid")

# Create side-by-side charts
fig, axes = plt.subplots(1, 2, figsize=(16, 6))


# --- Left Chart: Top 5 Risky Sellers ---
import matplotlib.pyplot as plt
import seaborn as sns

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# --- Left Chart: Top 5 Risky Sellers ---
sns.barplot(
    data=top_5_sellers,
    x='seller_risk_score',
    y='seller_id',
    color='crimson',  
    ax=axes[0]
)
axes[0].set_title(' Sellers to Suspend', fontsize=14)
axes[0].set_xlabel('Average Risk Score')
axes[0].set_ylabel('Seller ID')

for i, v in enumerate(top_5_sellers['seller_risk_score']):
    axes[0].text(v + 0.01, i, f"{v:.2f}", color='black', va='center')

# Objective: Identify top high-risk sellers
# Insight: Sellers with >0.75 average risk
# Recommendation: Suspend or audit listed sellers immediately

# --- Right Chart: Product Categories to Blacklist ---
sns.barplot(
    data=blacklist_categories,
    x='Complaint Rate',
    y='Product Category',
    color='darkred',  
    ax=axes[1]
)
axes[1].set_title(' Categories to Regulate', fontsize=14)
axes[1].set_xlabel('Complaint Rate')
axes[1].set_ylabel('Product Category')

for i, v in enumerate(blacklist_categories['Complaint Rate']):
    axes[1].text(v + 0.005, i, f"{v:.2%}", color='black', va='center')

# Objective: Highlight categories with frequent complaints
# Insight: Complaint rates above 5%
# Recommendation: Review suppliers or temporarily delist these items

# Layout and save
plt.suptitle(" Seller & Product Risk Summary Dashboard", fontsize=16, y=1.05)
plt.tight_layout()
plt.savefig("seller_product_risk_dashboard.png", bbox_inches='tight')
plt.show()


#  3. Actionable Strategies to Reduce Delivery Delays

strategies = [
    " 1. Reassign low-performing warehouses in high-delay regions.",
    " 2. Enforce strict dispatch timelines for sellers.",
    " 3. Use predictive routing to anticipate traffic and optimize delivery windows."
]

print("\n Strategies to Reduce Delivery Delays:")
for s in strategies:
    print(s)


#  4. Customer Trust Policy (3 bullet points)

trust_policy = [
    " 1. Transparent seller profiles and real-time order tracking.",
    " 2. 100% return guarantee on defective or delayed products.",
    " 3. 24/7 multilingual customer support for escalations and queries."
]

print("\n Customer Trust Policy:")
for point in trust_policy:
    print(point)


#  5. Seller Risk Framework Visual 

labels = ['Return Rate (40%)', 'Complaint Rate (40%)', 'Rating Penalty (15%)', 'Delay Penalty (15%)']
weights = [0.4, 0.4, 0.15, 0.15]

plt.figure(figsize=(8, 5))
plt.barh(labels, weights, color='crimson')
plt.title(' Seller Risk Framework: Weighted Components', fontsize=14)
plt.xlabel('Weight Contribution')
plt.tight_layout()
plt.savefig('seller_risk_framework.png')
plt.show()

print("\n Saved: seller_risk_framework.png")


# =======================================
# Business Curveballs 
# =======================================

print("\n Business Curveball 1: Lagos Warehouse Delay Data Issue")
print(" Issue: Lagos warehouse was over capacity â€” delay data may be unreliable.")

# Replace delays from Lagos zone with NaN
df['delivery_delay_adjusted'] = df['delivery_delay']
df.loc[df['warehouse_zone'].str.contains("Lagos", case=False, na=False), 'delivery_delay_adjusted'] = np.nan

# Recalculate average delay per region excluding Lagos zone
region_delay_adj = df.groupby('customer_region')['delivery_delay_adjusted'].mean().reset_index()
region_delay_adj = region_delay_adj.sort_values(by='delivery_delay_adjusted', ascending=False)

print("\n Adjusted Average Delivery Delay by Region (Lagos Excluded):")
print(region_delay_adj.to_string(index=False))


print("\n Business Curveball 2: Seller Caught Buying Fake Reviews")
print(" Issue: A top-rated seller was discovered posting fake reviews.")

# Identify sellers with high ratings but suspicious (short or duplicate) reviews
df['high_rating'] = df['customer_rating'] >= 4.5
df['short_review'] = df['review_length'] < 15

suspicious_reviews = df[df['high_rating'] & df['short_review']]
review_flags = suspicious_reviews.groupby('seller_id').agg({
    'customer_rating': 'mean',
    'review_length': 'median',
    'review_text': 'count'
}).rename(columns={
    'review_text': 'Suspicious Review Count'
}).reset_index()

print("\n Potential Fake Review Signal (High Rating + Very Short Review):")
print(review_flags.sort_values(by='Suspicious Review Count', ascending=False).head())


# Recommendation:
print("\n Recommendation: Incorporate review length and duplication into risk scoring to catch anomalies early.")


# These help;

# 1. Lagos Delay Handling: Removes Lagos from delay averages for clean analysis.

# 2. Fake Review Detection: Flags sellers with suspiciously short reviews despite high ratings.

# 3. Prints Insightful Comments: So that evaluator can see their thought process directly in output.



# =======================================
# Visualize Suspicious Sellers (High Rating + Short Reviews)
# =======================================


# Sample suspicious sellers from review_flags 
review_flags = df[df['review_length'] < 15].groupby(['seller_id', 'customer_rating']).agg({
    'review_length': 'mean',
    'review_text': 'count'
}).reset_index()

review_flags.rename(columns={
    'review_length': 'review_length',
    'review_text': 'Suspicious Review Count'
}, inplace=True)

# Top 5 suspicious sellers
top_suspicious = review_flags.sort_values(by='Suspicious Review Count', ascending=False).head(5)

# Chart
plt.figure(figsize=(10, 6))
ax = sns.barplot(
    data=top_suspicious,
    x='Suspicious Review Count',
    y='seller_id',
    color='darkred'  
)

plt.title('Top 5 Suspicious Sellers with Potential Fake Reviews', fontsize=13)
plt.xlabel('Suspicious Review Count')
plt.ylabel('Seller ID')

# Annotate values
for i, v in enumerate(top_suspicious['Suspicious Review Count']):
    ax.text(v + 0.3, i, f'{v}', va='center', color='black')

plt.tight_layout()
plt.savefig('suspicious_sellers_fake_reviews.png')
plt.show()

#  Summary Text
print("\n Chart Objective: Identify sellers with unusually short & high-rated reviews.")
print(" Insight: Sellers may be attempting to inflate ratings with minimal text reviews.")
print(" Recommendation: Flag sellers with repeat short reviews for audit or auto-risk increase.")





